{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uZ3Rb6JRfF3"
   },
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s5yo2Dqjqef"
   },
   "source": [
    "Please, note that this notebook is intended to be run in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkMH-1WClvTd"
   },
   "outputs": [],
   "source": [
    "# Mount drive and define path to the data folder (from your Google Drive)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "datapath = 'drive/MyDrive/data/amazon_reviews/All_Beauty/'\n",
    "train_file = 'training.pkl'\n",
    "test_file = 'test.pkl'\n",
    "meta_file = 'meta_All_Beauty.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIYb2Pxi6_Ie"
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Load the [metadata file](https://nijianmo.github.io/amazon/index.html) and discard any item that was not rated by our subset of users (nor in training or test sets). Apply preprocessing (stemming and stopwords removal) to clean up the text from the \"title\". Report the vocabulary size before and after the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKbUQSlc65kO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load TRAIN and TEST sets \n",
    "\n",
    "# Load the METADATA (ITEMS)\n",
    "\n",
    "# Discard duplicates\n",
    "\n",
    "# Discard items that weren't rated by our subset of users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rd0RbH-k9y1H"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "# <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Blr1jgoHLbFU"
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Representation in vector spaces.\n",
    "\n",
    "## 2.1\n",
    "\n",
    "Represent all the products from Exercise 1 in a TF-IDF space. Interpret the meaning of the TF-IDF matrix dimensions.\n",
    "\n",
    "Tip: You may use the library [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDndolvDLznV"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gnTVM0EV_2d"
   },
   "source": [
    "## 2.2\n",
    "\n",
    "Compute and the cosine similarity between products with asin 'B000FI4S1E', 'B000LIBUBY' and 'B000W0C07Y'. Take a look at their features to see whether results make sense with their characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zO_OHMY8PWbO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0K8jRhWhZQWe"
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "Representation in vector spaces with contextual Word Embeddings.\n",
    "\n",
    "## 3.1.\n",
    "\n",
    "Represent all the products from Exercise 1 in a vector space using embeddings from a pre-trained BERT model. The final embedding of a product should be the average of the word embeddings from all the words in the 'title'. What is the vocabulary size of the model? What are the dimensions of the last hidden state?\n",
    "\n",
    "Tip: you may install the transformers library and use their pretrained [BERT model uncased](https://huggingface.co/bert-base-uncased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHIjJ-LbTB3H"
   },
   "outputs": [],
   "source": [
    "# LOAD TRANSFORMER\n",
    "\"\"\"\n",
    "If you plan on using a pretrained model, itâ€™s important to use the associated \n",
    "pretrained tokenizer: it will split the text you give it in tokens the same way\n",
    "for the pretraining corpus, and it will use the same correspondence\n",
    "token to index (that we usually call a vocab) as during pretraining.\n",
    "\"\"\"\n",
    "\n",
    "# % pip install transformers\n",
    "import torch\n",
    "import transformers\n",
    "assert transformers.__version__ > '4.0.0'\n",
    "\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "# set-up environment\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "modelname = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(modelname)\n",
    "model = BertModel.from_pretrained(modelname).to(DEVICE)\n",
    "\n",
    "# Print out the vocabulary size\n",
    "# <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_Symyv5U07x"
   },
   "outputs": [],
   "source": [
    "# REPRESENT PRODUCTS IN A VECTOR SPACE\n",
    "\n",
    "\n",
    "def batch_encoding(sentences):\n",
    "    # Since we're using padding, we need to provide the attention masks to our\n",
    "    # model. Otherwise it doesn't know which tokens it should not attend to. \n",
    "    inputs = # <YOUR CODE HERE>\n",
    "    # print(inputs) # Look at the padding and attention_mask\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    last_hidden_states = # <YOUR CODE HERE>\n",
    "\n",
    "    return inputs, last_hidden_states\n",
    "  \n",
    "encoded_inputs, title_last_hidden_states = batch_encoding( # <YOUR CODE HERE> )\n",
    "\n",
    "\"\"\"\n",
    "Note that the control token [CLS] has been added \n",
    "at the beginning of each sentence, and [SEP] at the end. \n",
    "\"\"\"\n",
    "\n",
    "# Now, let's mask out the padding tokens and compute the embedding vector of each product\n",
    "\n",
    "# <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwRBr2HP0Zdt"
   },
   "source": [
    "## 3.2.\n",
    "\n",
    "Compute and the cosine similarity between products with asin 'B000FI4S1E', 'B000LIBUBY' and 'B000W0C07Y'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaHxSLHqItNs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Session_4-Text-Representation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
