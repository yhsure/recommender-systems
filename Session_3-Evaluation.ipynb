{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d23701c",
   "metadata": {},
   "source": [
    "# Evaluation of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcface96",
   "metadata": {},
   "source": [
    "Based on the same dataset used on previous weeks, let us evaluate the Collaborative Filtering (CF) models implemented last week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5b50d",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Load the test set and the predictions made with both Collaborative Filtering models in the previous session. \n",
    "2. Detect those users which are in the training set but not in the test set. Remove their predictions before evaluating the systems.\n",
    "3. Report the Root Mean Square Error (RMSE) for both CF models defined in the previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# TEST\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# PREDICTIONS\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Detect users from training set that are not in test\n",
    "nb_users = set([pred.uid for pred in pred_nb_list])\n",
    "lf_users = set([pred.uid for pred in pred_lf_list])\n",
    "nb_users_in_pred_but_not_in_test = list(nb_users.difference(set(df_test['reviewerID'])))\n",
    "lf_users_in_pred_but_not_in_test = list(lf_users.difference(set(df_test['reviewerID'])))\n",
    "assert nb_users_in_pred_but_not_in_test == lf_users_in_pred_but_not_in_test\n",
    "print(f\"There are {len(lf_users_in_pred_but_not_in_test)} users in the training set that are not in the test set.\")\n",
    "\n",
    "# Remove these users' predictions for evaluation\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c3d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eedf3c25",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Define a general method to get the top-k recommendations for each user. Print the top-k with k={5, 10} recommendations for the user with ID 'ARARUVZ8RUF5T' and its estimated ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95e3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03325a14",
   "metadata": {},
   "source": [
    "## Excercise 3\n",
    "Report Precision@k (P@k), MAP@k and the MRR@k with k={5, 10, 20} averaged across users for both CF systems. When computing precision, we consider as relevant items those with an observed rating >= 4.0 (i.e., those items from the test set with a rating >= 4.0). Reflect on the differences obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26be2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bf4c50b",
   "metadata": {},
   "source": [
    "## Excercise 4\n",
    "\n",
    "Based on the top-5, top-10 and top-20 predictions from Exercise 2, compute the systemsâ€™ hit rate averaged over the total number of users in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062588c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
