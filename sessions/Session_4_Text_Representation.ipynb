{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uZ3Rb6JRfF3"
   },
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s5yo2Dqjqef"
   },
   "source": [
    "Please, note that this notebook is intended to be run in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27384,
     "status": "ok",
     "timestamp": 1646991245298,
     "user": {
      "displayName": "nd d",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10402811649730711830"
     },
     "user_tz": -60
    },
    "id": "dkMH-1WClvTd",
    "outputId": "0267122b-6723-4fdb-bc5c-2aad1271ddb3"
   },
   "outputs": [],
   "source": [
    "train_file ='train.pkl'\n",
    "test_file = 'test.pkl'\n",
    "meta_file = 'meta_All_Beauty.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIYb2Pxi6_Ie"
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Load the [metadata file](https://nijianmo.github.io/amazon/index.html) and discard any item that was not rated by our subset of users (nor in training or test sets). Apply preprocessing (stemming and stopwords removal) to clean up the text from the \"title\". Report the vocabulary size before and after the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping dups: 32892\n",
      "After: 32488\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "train = pd.read_pickle(\"train.pkl\")\n",
    "test = pd.read_pickle(\"test.pkl\")\n",
    "\n",
    "\n",
    "meta = getDF(meta_file)\n",
    "print(\"Before dropping dups:\", len(meta))\n",
    "meta = meta.drop_duplicates([\"asin\"], keep=\"last\")\n",
    "print(\"After:\", len(meta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1646991258888,
     "user": {
      "displayName": "nd d",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10402811649730711830"
     },
     "user_tz": -60
    },
    "id": "RAlxFFO8KzwF",
    "outputId": "ee2ac727-d10d-4a46-d451-20665cfcad82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowed_asins = np.unique(np.array([*train.asin.to_list(), *test.asin.to_list()]))\n",
    "meta = meta[meta[\"asin\"].isin(allowed_asins)].reset_index(drop=True)\n",
    "\n",
    "len(allowed_asins), len(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1164,
     "status": "ok",
     "timestamp": 1646991260046,
     "user": {
      "displayName": "nd d",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10402811649730711830"
     },
     "user_tz": -60
    },
    "id": "Rd0RbH-k9y1H",
    "outputId": "b1500f94-152d-495d-ffc1-7f8683ae21a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ayaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'hi', 'jumped']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "s = PorterStemmer()\n",
    "\n",
    "[wnl.lemmatize(w) for w in [\"hello\", \"hi\", \"jumped\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1646991268082,
     "user": {
      "displayName": "nd d",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10402811649730711830"
     },
     "user_tz": -60
    },
    "id": "hfwp5TMmOE5B",
    "outputId": "79cb90c4-262a-4b94-9571-978d0eee8d2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                aqua velva shave classic ice blue ounc\n",
       "1                citr shine moistur burst shampoo fl oz\n",
       "2                                   nar blush taj mahal\n",
       "3        avalon organ wrinkl therapi coq cleans milk oz\n",
       "4                          zum zum bar anis lavend ounc\n",
       "                            ...                        \n",
       "79                     ultim bodi lotion michael kor oz\n",
       "80                     dolc gabbana compact parfum ounc\n",
       "81    colgat kid maximum caviti protect pump toothpa...\n",
       "82    bali secret natur deodor organ vegan women men...\n",
       "83                          essi gel coutur nail polish\n",
       "Name: words, Length: 84, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pattern.en import lemma\n",
    "\n",
    "# based on https://stackoverflow.com/questions/45670532/stemming-words-with-nltk-python \n",
    "def cleanWords(x, stemmer=\"porter\"):\n",
    "    #choose stemming/lemmatization \n",
    "    if stemmer == \"lemma\":\n",
    "        stem = lambda w: lemma(w)\n",
    "    else: \n",
    "        f = PorterStemmer()\n",
    "        stem = lambda w: f.stem(w)\n",
    "        \n",
    "    txt = BeautifulSoup(x).get_text()              #remove html\n",
    "    letters = re.sub(\"[^a-z]\", \" \", txt.lower())   #lowercase and only letters\n",
    "    words   = word_tokenize(letters)               #tokenize\n",
    "    stops = set(stopwords.words(\"english\"))        #define stopwords \n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    stems = [stem(w) for w in meaningful_words]    #stem/lemmatize\n",
    "    \n",
    "    return ' '.join([w for w in stems if len(w)>1])\n",
    "\n",
    "df = meta.copy()\n",
    "df[\"words\"] = df[\"title\"].apply(lambda x: cleanWords(x))\n",
    "\n",
    "df[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646991269520,
     "user": {
      "displayName": "nd d",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10402811649730711830"
     },
     "user_tz": -60
    },
    "id": "YRI-6NAMYNmS",
    "outputId": "a405e699-c344-42d3-f13b-ce22960cf341"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words in vocab (excludes 1-letter stems)\n",
    "len(set(' '.join(df[\"words\"]).split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Blr1jgoHLbFU"
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Representation in vector spaces.\n",
    "\n",
    "## 2.1\n",
    "\n",
    "Represent all the products from Exercise 1 in a TF-IDF space. Interpret the meaning of the TF-IDF matrix dimensions.\n",
    "\n",
    "Tip: You may use the library [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 946,
     "status": "ok",
     "timestamp": 1646992313559,
     "user": {
      "displayName": "nd d",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10402811649730711830"
     },
     "user_tz": -60
    },
    "id": "vDndolvDLznV",
    "outputId": "d9424cb9-62eb-4fe9-d4d2-bc45e3491ae9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>advanc</th>\n",
       "      <th>aerosol</th>\n",
       "      <th>age</th>\n",
       "      <th>ageless</th>\n",
       "      <th>air</th>\n",
       "      <th>allergen</th>\n",
       "      <th>almond</th>\n",
       "      <th>american</th>\n",
       "      <th>andal</th>\n",
       "      <th>...</th>\n",
       "      <th>wintergreen</th>\n",
       "      <th>wiseway</th>\n",
       "      <th>witch</th>\n",
       "      <th>women</th>\n",
       "      <th>wood</th>\n",
       "      <th>work</th>\n",
       "      <th>wrinkl</th>\n",
       "      <th>yardley</th>\n",
       "      <th>youth</th>\n",
       "      <th>zum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B0000530HU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00006L9LC</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00021DJ32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B0002JHI1I</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B0006O10P4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.782655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B019LAI4HU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B019V2KYZS</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01BNEYGQU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01DKQAXC0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01E7UKR38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 394 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            action  advanc  aerosol  age  ageless  air  allergen  almond  \\\n",
       "asin                                                                       \n",
       "B0000530HU     0.0     0.0      0.0  0.0      0.0  0.0       0.0     0.0   \n",
       "B00006L9LC     0.0     0.0      0.0  0.0      0.0  0.0       0.0     0.0   \n",
       "B00021DJ32     0.0     0.0      0.0  0.0      0.0  0.0       0.0     0.0   \n",
       "B0002JHI1I     0.0     0.0      0.0  0.0      0.0  0.0       0.0     0.0   \n",
       "B0006O10P4     0.0     0.0      0.0  0.0      0.0  0.0       0.0     0.0   \n",
       "...            ...     ...      ...  ...      ...  ...       ...     ...   \n",
       "B019LAI4HU     0.0     0.0      0.0  0.0      0.0  0.0       0.0     0.0   \n",
       "B019V2KYZS     0.0     0.0      0.0  0.0      0.0  0.0       0.0     0.0   \n",
       "B01BNEYGQU     0.0     0.0      0.0  0.0      0.0  0.0       0.0     0.0   \n",
       "B01DKQAXC0     0.0     0.0      0.0  0.0      0.0  0.0       0.0     0.0   \n",
       "B01E7UKR38     0.0     0.0      0.0  0.0      0.0  0.0       0.0     0.0   \n",
       "\n",
       "            american  andal  ...  wintergreen  wiseway  witch     women  wood  \\\n",
       "asin                         ...                                                \n",
       "B0000530HU       0.0    0.0  ...          0.0      0.0    0.0  0.000000   0.0   \n",
       "B00006L9LC       0.0    0.0  ...          0.0      0.0    0.0  0.000000   0.0   \n",
       "B00021DJ32       0.0    0.0  ...          0.0      0.0    0.0  0.000000   0.0   \n",
       "B0002JHI1I       0.0    0.0  ...          0.0      0.0    0.0  0.000000   0.0   \n",
       "B0006O10P4       0.0    0.0  ...          0.0      0.0    0.0  0.000000   0.0   \n",
       "...              ...    ...  ...          ...      ...    ...       ...   ...   \n",
       "B019LAI4HU       0.0    0.0  ...          0.0      0.0    0.0  0.000000   0.0   \n",
       "B019V2KYZS       0.0    0.0  ...          0.0      0.0    0.0  0.000000   0.0   \n",
       "B01BNEYGQU       0.0    0.0  ...          0.0      0.0    0.0  0.000000   0.0   \n",
       "B01DKQAXC0       0.0    0.0  ...          0.0      0.0    0.0  0.227715   0.0   \n",
       "B01E7UKR38       0.0    0.0  ...          0.0      0.0    0.0  0.000000   0.0   \n",
       "\n",
       "            work    wrinkl  yardley  youth       zum  \n",
       "asin                                                  \n",
       "B0000530HU   0.0  0.000000      0.0    0.0  0.000000  \n",
       "B00006L9LC   0.0  0.000000      0.0    0.0  0.000000  \n",
       "B00021DJ32   0.0  0.000000      0.0    0.0  0.000000  \n",
       "B0002JHI1I   0.0  0.351338      0.0    0.0  0.000000  \n",
       "B0006O10P4   0.0  0.000000      0.0    0.0  0.782655  \n",
       "...          ...       ...      ...    ...       ...  \n",
       "B019LAI4HU   0.0  0.000000      0.0    0.0  0.000000  \n",
       "B019V2KYZS   0.0  0.000000      0.0    0.0  0.000000  \n",
       "B01BNEYGQU   0.0  0.000000      0.0    0.0  0.000000  \n",
       "B01DKQAXC0   0.0  0.000000      0.0    0.0  0.000000  \n",
       "B01E7UKR38   0.0  0.000000      0.0    0.0  0.000000  \n",
       "\n",
       "[84 rows x 394 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(df[\"words\"])\n",
    "tfidf = tfidf_vectorizer.fit_transform(df[\"words\"])\n",
    "names = tfidf_vectorizer.get_feature_names_out()\n",
    "arr = tfidf.toarray() \n",
    "\n",
    "df2 = pd.DataFrame(arr, index=df[\"asin\"], columns=names)\n",
    "df2.to_pickle(\"asin_tfidf.pkl\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gnTVM0EV_2d"
   },
   "source": [
    "## 2.2\n",
    "\n",
    "Compute and the cosine similarity between products with asin 'B000FI4S1E', 'B000LIBUBY' and 'B000W0C07Y'. Take a look at their features to see whether results make sense with their characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B000FI4S1E</th>\n",
       "      <th>B000LIBUBY</th>\n",
       "      <th>B000W0C07Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B000FI4S1E</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B000LIBUBY</th>\n",
       "      <td>0.034</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B000W0C07Y</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.441</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            B000FI4S1E  B000LIBUBY  B000W0C07Y\n",
       "B000FI4S1E       1.000       0.034       0.025\n",
       "B000LIBUBY       0.034       1.000       0.441\n",
       "B000W0C07Y       0.025       0.441       1.000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "asins = [\"B000FI4S1E\", \"B000LIBUBY\", \"B000W0C07Y\"]\n",
    "sims = pd.DataFrame(cosine_similarity(df2.loc[asins]).round(3), index=asins, columns=asins)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>rank</th>\n",
       "      <th>details</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>asin</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Fruits &amp;amp; Passion SOLSTIS Refreshing Showe...</td>\n",
       "      <td>Fruits &amp;amp; Passion Blue Refreshing Shower Ge...</td>\n",
       "      <td>Fruits &amp; Passion</td>\n",
       "      <td>2,539,624 in Beauty &amp; Personal Care (</td>\n",
       "      <td>{'Shipping Weight:': '8 ounces', 'ASIN: ': 'B0...</td>\n",
       "      <td>All Beauty</td>\n",
       "      <td>B000FI4S1E</td>\n",
       "      <td>fruit passion blue refresh shower gel fl oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[&lt;li&gt;A brilliant effervescent fragrance for yo...</td>\n",
       "      <td>Fresh Eau de Parfum, Sugar Lemon, 3.4 oz</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>572,901 in Beauty &amp; Personal Care (</td>\n",
       "      <td>{'\n",
       "    Item Weight: \n",
       "    ': '3.36 ounces', 'Sh...</td>\n",
       "      <td>All Beauty</td>\n",
       "      <td>B000LIBUBY</td>\n",
       "      <td>fresh eau de parfum sugar lemon oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[, For Women]</td>\n",
       "      <td>Sex In The City Kiss by Instyle Parfums Eau De...</td>\n",
       "      <td>Instyle Parfums</td>\n",
       "      <td>616,259 in Beauty &amp; Personal Care (</td>\n",
       "      <td>{'\n",
       "    Item Weight: \n",
       "    ': '0.8 ounces', 'Shi...</td>\n",
       "      <td>All Beauty</td>\n",
       "      <td>B000W0C07Y</td>\n",
       "      <td>sex citi kiss instyl parfum eau de parfum spra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          description  \\\n",
       "6   [Fruits &amp; Passion SOLSTIS Refreshing Showe...   \n",
       "10  [<li>A brilliant effervescent fragrance for yo...   \n",
       "17                                      [, For Women]   \n",
       "\n",
       "                                                title             brand  \\\n",
       "6   Fruits &amp; Passion Blue Refreshing Shower Ge...  Fruits & Passion   \n",
       "10           Fresh Eau de Parfum, Sugar Lemon, 3.4 oz             Fresh   \n",
       "17  Sex In The City Kiss by Instyle Parfums Eau De...   Instyle Parfums   \n",
       "\n",
       "                                     rank  \\\n",
       "6   2,539,624 in Beauty & Personal Care (   \n",
       "10    572,901 in Beauty & Personal Care (   \n",
       "17    616,259 in Beauty & Personal Care (   \n",
       "\n",
       "                                              details    main_cat        asin  \\\n",
       "6   {'Shipping Weight:': '8 ounces', 'ASIN: ': 'B0...  All Beauty  B000FI4S1E   \n",
       "10  {'\n",
       "    Item Weight: \n",
       "    ': '3.36 ounces', 'Sh...  All Beauty  B000LIBUBY   \n",
       "17  {'\n",
       "    Item Weight: \n",
       "    ': '0.8 ounces', 'Shi...  All Beauty  B000W0C07Y   \n",
       "\n",
       "                                                words  \n",
       "6         fruit passion blue refresh shower gel fl oz  \n",
       "10                 fresh eau de parfum sugar lemon oz  \n",
       "17  sex citi kiss instyl parfum eau de parfum spra...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the two perfumes are similar in tf-idf space, while the shower gel shares less similarity \n",
    "df[df[\"asin\"].isin(asins)][[\"description\", \"title\", \"brand\", \"rank\", \"details\", \"main_cat\", \"asin\", \"words\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained word2vec (Gogle News 300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "# https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794\n",
    "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# - Easy to compute\n",
    "# - You have some basic metric to extract the most descriptive terms in a document\n",
    "# - You can easily compute the similarity between 2 documents using it\n",
    "\n",
    "# Disadvantages:\n",
    "# - TF-IDF is based on the bag-of-words (BoW) model, therefore it does not capture position in text, semantics, co-occurrences in different documents, etc.\n",
    "# - For this reason, TF-IDF is only useful as a lexical level feature\n",
    "# - Cannot capture semantics (e.g. as compared to topic models, word embeddings)\n",
    "\n",
    "\n",
    "def cleanWords2(x):\n",
    "    txt = BeautifulSoup(x).get_text()              #remove html\n",
    "    letters = re.sub(\"[^a-z]\", \" \", txt.lower())   #lowercase and only letters\n",
    "    words   = word_tokenize(letters)               #tokenize\n",
    "    stops = set(stopwords.words(\"english\"))        #define stopwords \n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    stems = [lemma(w) for w in meaningful_words]    #stem/lemmatize\n",
    "    return ' '.join([w for w in stems if len(w)>1 and w in word2vec_vectors.index_to_key])\n",
    "\n",
    "df = meta.copy()\n",
    "df[\"words\"] = df[\"title\"].apply(lambda x: cleanWords2(x))\n",
    "\n",
    "len(set(' '.join(df[\"words\"]).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B0000530HU</th>\n",
       "      <td>-0.064453</td>\n",
       "      <td>0.015808</td>\n",
       "      <td>0.087301</td>\n",
       "      <td>0.156169</td>\n",
       "      <td>-0.024231</td>\n",
       "      <td>0.083649</td>\n",
       "      <td>0.074621</td>\n",
       "      <td>-0.236328</td>\n",
       "      <td>0.081584</td>\n",
       "      <td>0.167074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006388</td>\n",
       "      <td>-0.121243</td>\n",
       "      <td>-0.064653</td>\n",
       "      <td>-0.003204</td>\n",
       "      <td>0.110291</td>\n",
       "      <td>-0.121277</td>\n",
       "      <td>0.056742</td>\n",
       "      <td>0.026123</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>-0.108927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00006L9LC</th>\n",
       "      <td>-0.037109</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>0.008291</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.037287</td>\n",
       "      <td>-0.238973</td>\n",
       "      <td>0.050049</td>\n",
       "      <td>0.190755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074056</td>\n",
       "      <td>-0.020447</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>-0.130086</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>-0.087158</td>\n",
       "      <td>-0.084208</td>\n",
       "      <td>0.034382</td>\n",
       "      <td>-0.002218</td>\n",
       "      <td>-0.054127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00021DJ32</th>\n",
       "      <td>-0.005325</td>\n",
       "      <td>0.084473</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.202393</td>\n",
       "      <td>-0.100220</td>\n",
       "      <td>-0.060181</td>\n",
       "      <td>0.141602</td>\n",
       "      <td>-0.001938</td>\n",
       "      <td>-0.039429</td>\n",
       "      <td>0.140015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054565</td>\n",
       "      <td>0.029846</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>-0.049034</td>\n",
       "      <td>-0.091888</td>\n",
       "      <td>-0.162369</td>\n",
       "      <td>-0.091736</td>\n",
       "      <td>0.048187</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>0.219971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B0002JHI1I</th>\n",
       "      <td>-0.055629</td>\n",
       "      <td>0.149658</td>\n",
       "      <td>0.059636</td>\n",
       "      <td>0.166574</td>\n",
       "      <td>-0.071777</td>\n",
       "      <td>0.182216</td>\n",
       "      <td>0.039246</td>\n",
       "      <td>-0.304408</td>\n",
       "      <td>-0.035784</td>\n",
       "      <td>0.102295</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058446</td>\n",
       "      <td>-0.105325</td>\n",
       "      <td>-0.039400</td>\n",
       "      <td>0.013044</td>\n",
       "      <td>-0.023751</td>\n",
       "      <td>-0.016785</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>0.132551</td>\n",
       "      <td>0.160993</td>\n",
       "      <td>-0.078753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B0006O10P4</th>\n",
       "      <td>0.120687</td>\n",
       "      <td>-0.180176</td>\n",
       "      <td>-0.060542</td>\n",
       "      <td>0.074137</td>\n",
       "      <td>0.035075</td>\n",
       "      <td>0.212133</td>\n",
       "      <td>0.170166</td>\n",
       "      <td>-0.273600</td>\n",
       "      <td>0.055583</td>\n",
       "      <td>0.221598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078613</td>\n",
       "      <td>-0.063639</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.115031</td>\n",
       "      <td>0.042175</td>\n",
       "      <td>-0.046038</td>\n",
       "      <td>0.019613</td>\n",
       "      <td>0.010091</td>\n",
       "      <td>0.196696</td>\n",
       "      <td>0.172689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B019LAI4HU</th>\n",
       "      <td>-0.116750</td>\n",
       "      <td>0.032349</td>\n",
       "      <td>0.085856</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>-0.033254</td>\n",
       "      <td>0.094320</td>\n",
       "      <td>0.042837</td>\n",
       "      <td>-0.283081</td>\n",
       "      <td>0.089238</td>\n",
       "      <td>0.036402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029602</td>\n",
       "      <td>-0.067546</td>\n",
       "      <td>0.027952</td>\n",
       "      <td>0.027059</td>\n",
       "      <td>-0.156708</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>-0.133169</td>\n",
       "      <td>-0.026825</td>\n",
       "      <td>-0.084595</td>\n",
       "      <td>0.042480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B019V2KYZS</th>\n",
       "      <td>0.032776</td>\n",
       "      <td>-0.047424</td>\n",
       "      <td>0.035645</td>\n",
       "      <td>0.217041</td>\n",
       "      <td>-0.029999</td>\n",
       "      <td>0.160461</td>\n",
       "      <td>0.098083</td>\n",
       "      <td>-0.217773</td>\n",
       "      <td>0.115356</td>\n",
       "      <td>0.181702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091675</td>\n",
       "      <td>-0.115889</td>\n",
       "      <td>-0.016052</td>\n",
       "      <td>-0.040283</td>\n",
       "      <td>-0.019806</td>\n",
       "      <td>0.023926</td>\n",
       "      <td>0.034149</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>0.055420</td>\n",
       "      <td>0.058533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01BNEYGQU</th>\n",
       "      <td>-0.010361</td>\n",
       "      <td>-0.005264</td>\n",
       "      <td>0.037292</td>\n",
       "      <td>0.078068</td>\n",
       "      <td>-0.026947</td>\n",
       "      <td>0.114807</td>\n",
       "      <td>0.034210</td>\n",
       "      <td>-0.139999</td>\n",
       "      <td>0.109718</td>\n",
       "      <td>0.105042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013245</td>\n",
       "      <td>-0.066833</td>\n",
       "      <td>-0.160889</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>-0.101013</td>\n",
       "      <td>0.054108</td>\n",
       "      <td>-0.060120</td>\n",
       "      <td>0.020233</td>\n",
       "      <td>-0.002808</td>\n",
       "      <td>-0.116760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01DKQAXC0</th>\n",
       "      <td>-0.025289</td>\n",
       "      <td>0.069427</td>\n",
       "      <td>0.044976</td>\n",
       "      <td>0.144896</td>\n",
       "      <td>-0.053573</td>\n",
       "      <td>0.020009</td>\n",
       "      <td>-0.014752</td>\n",
       "      <td>-0.181085</td>\n",
       "      <td>0.061490</td>\n",
       "      <td>0.161411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>-0.040436</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.021389</td>\n",
       "      <td>-0.001712</td>\n",
       "      <td>-0.049068</td>\n",
       "      <td>-0.065881</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>0.063789</td>\n",
       "      <td>0.061069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01E7UKR38</th>\n",
       "      <td>0.021729</td>\n",
       "      <td>0.081909</td>\n",
       "      <td>-0.101685</td>\n",
       "      <td>-0.169434</td>\n",
       "      <td>-0.018127</td>\n",
       "      <td>0.085814</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>-0.296631</td>\n",
       "      <td>-0.083130</td>\n",
       "      <td>0.129578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025757</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>-0.134399</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>-0.057312</td>\n",
       "      <td>0.059860</td>\n",
       "      <td>0.134888</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>-0.026123</td>\n",
       "      <td>0.055038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5    \\\n",
       "asin                                                                     \n",
       "B0000530HU -0.064453  0.015808  0.087301  0.156169 -0.024231  0.083649   \n",
       "B00006L9LC -0.037109  0.143555  0.008291  0.017334  0.072917  0.038086   \n",
       "B00021DJ32 -0.005325  0.084473  0.081787  0.202393 -0.100220 -0.060181   \n",
       "B0002JHI1I -0.055629  0.149658  0.059636  0.166574 -0.071777  0.182216   \n",
       "B0006O10P4  0.120687 -0.180176 -0.060542  0.074137  0.035075  0.212133   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "B019LAI4HU -0.116750  0.032349  0.085856  0.114583 -0.033254  0.094320   \n",
       "B019V2KYZS  0.032776 -0.047424  0.035645  0.217041 -0.029999  0.160461   \n",
       "B01BNEYGQU -0.010361 -0.005264  0.037292  0.078068 -0.026947  0.114807   \n",
       "B01DKQAXC0 -0.025289  0.069427  0.044976  0.144896 -0.053573  0.020009   \n",
       "B01E7UKR38  0.021729  0.081909 -0.101685 -0.169434 -0.018127  0.085814   \n",
       "\n",
       "                 6         7         8         9    ...       290       291  \\\n",
       "asin                                                ...                       \n",
       "B0000530HU  0.074621 -0.236328  0.081584  0.167074  ... -0.006388 -0.121243   \n",
       "B00006L9LC  0.037287 -0.238973  0.050049  0.190755  ...  0.074056 -0.020447   \n",
       "B00021DJ32  0.141602 -0.001938 -0.039429  0.140015  ...  0.054565  0.029846   \n",
       "B0002JHI1I  0.039246 -0.304408 -0.035784  0.102295  ... -0.058446 -0.105325   \n",
       "B0006O10P4  0.170166 -0.273600  0.055583  0.221598  ...  0.078613 -0.063639   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "B019LAI4HU  0.042837 -0.283081  0.089238  0.036402  ... -0.029602 -0.067546   \n",
       "B019V2KYZS  0.098083 -0.217773  0.115356  0.181702  ... -0.091675 -0.115889   \n",
       "B01BNEYGQU  0.034210 -0.139999  0.109718  0.105042  ... -0.013245 -0.066833   \n",
       "B01DKQAXC0 -0.014752 -0.181085  0.061490  0.161411  ... -0.000699 -0.040436   \n",
       "B01E7UKR38  0.015259 -0.296631 -0.083130  0.129578  ... -0.025757  0.018333   \n",
       "\n",
       "                 292       293       294       295       296       297  \\\n",
       "asin                                                                     \n",
       "B0000530HU -0.064653 -0.003204  0.110291 -0.121277  0.056742  0.026123   \n",
       "B00006L9LC  0.041300 -0.130086  0.007345 -0.087158 -0.084208  0.034382   \n",
       "B00021DJ32  0.001984 -0.049034 -0.091888 -0.162369 -0.091736  0.048187   \n",
       "B0002JHI1I -0.039400  0.013044 -0.023751 -0.016785  0.004395  0.132551   \n",
       "B0006O10P4  0.029297  0.115031  0.042175 -0.046038  0.019613  0.010091   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "B019LAI4HU  0.027952  0.027059 -0.156708 -0.063965 -0.133169 -0.026825   \n",
       "B019V2KYZS -0.016052 -0.040283 -0.019806  0.023926  0.034149 -0.015625   \n",
       "B01BNEYGQU -0.160889 -0.022095 -0.101013  0.054108 -0.060120  0.020233   \n",
       "B01DKQAXC0 -0.052290 -0.021389 -0.001712 -0.049068 -0.065881  0.033488   \n",
       "B01E7UKR38 -0.134399  0.012772 -0.057312  0.059860  0.134888 -0.076923   \n",
       "\n",
       "                 298       299  \n",
       "asin                            \n",
       "B0000530HU  0.023010 -0.108927  \n",
       "B00006L9LC -0.002218 -0.054127  \n",
       "B00021DJ32  0.019043  0.219971  \n",
       "B0002JHI1I  0.160993 -0.078753  \n",
       "B0006O10P4  0.196696  0.172689  \n",
       "...              ...       ...  \n",
       "B019LAI4HU -0.084595  0.042480  \n",
       "B019V2KYZS  0.055420  0.058533  \n",
       "B01BNEYGQU -0.002808 -0.116760  \n",
       "B01DKQAXC0  0.063789  0.061069  \n",
       "B01E7UKR38 -0.026123  0.055038  \n",
       "\n",
       "[84 rows x 300 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vector(x):\n",
    "    x = x.split(' ')\n",
    "    i = 0\n",
    "    doc_embedding = np.zeros((300,))\n",
    "    for word in x:\n",
    "        i += 1\n",
    "        doc_embedding += word2vec_vectors[word]\n",
    "    return (doc_embedding/i)\n",
    "\n",
    "arr = [np.array(x) for x in df[\"words\"].apply(get_vector).to_numpy()]\n",
    "df3 = pd.DataFrame(arr, index=df[\"asin\"], columns=range(300))\n",
    "df3.to_pickle(\"asin_tfidf2.pkl\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "asdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0K8jRhWhZQWe"
   },
   "source": [
    "OPTIONAL\n",
    "\n",
    "|\n",
    "\n",
    "|\n",
    "\n",
    "|\n",
    "\n",
    "|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exercise 3\n",
    "\n",
    "Representation in vector spaces with contextual Word Embeddings.\n",
    "\n",
    "## 3.1.\n",
    "\n",
    "Represent all the products from Exercise 1 in a vector space using embeddings from a pre-trained BERT model. The final embedding of a product should be the average of the word embeddings from all the words in the 'title'. What is the vocabulary size of the model? What are the dimensions of the last hidden state?\n",
    "\n",
    "Tip: you may install the transformers library and use their pretrained [BERT model uncased](https://huggingface.co/bert-base-uncased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHIjJ-LbTB3H"
   },
   "outputs": [],
   "source": [
    "# LOAD TRANSFORMER\n",
    "\"\"\"\n",
    "If you plan on using a pretrained model, it’s important to use the associated \n",
    "pretrained tokenizer: it will split the text you give it in tokens the same way\n",
    "for the pretraining corpus, and it will use the same correspondence\n",
    "token to index (that we usually call a vocab) as during pretraining.\n",
    "\"\"\"\n",
    "\n",
    "# % pip install transformers\n",
    "import torch\n",
    "import transformers\n",
    "assert transformers.__version__ > '4.0.0'\n",
    "\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "# set-up environment\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "modelname = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(modelname)\n",
    "model = BertModel.from_pretrained(modelname).to(DEVICE)\n",
    "\n",
    "# Print out the vocabulary size\n",
    "# <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0z7j8gg74FG"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_Symyv5U07x"
   },
   "outputs": [],
   "source": [
    "# REPRESENT PRODUCTS IN A VECTOR SPACE\n",
    "\n",
    "\n",
    "def batch_encoding(sentences):\n",
    "    # Since we're using padding, we need to provide the attention masks to our\n",
    "    # model. Otherwise it doesn't know which tokens it should not attend to. \n",
    "    inputs = # <YOUR CODE HERE>\n",
    "    # print(inputs) # Look at the padding and attention_mask\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    last_hidden_states = # <YOUR CODE HERE>\n",
    "\n",
    "    return inputs, last_hidden_states\n",
    "  \n",
    "encoded_inputs, title_last_hidden_states = batch_encoding( # <YOUR CODE HERE> )\n",
    "\n",
    "\"\"\"\n",
    "Note that the control token [CLS] has been added \n",
    "at the beginning of each sentence, and [SEP] at the end. \n",
    "\"\"\"\n",
    "\n",
    "# Now, let's mask out the padding tokens and compute the embedding vector of each product\n",
    "\n",
    "# <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwRBr2HP0Zdt"
   },
   "source": [
    "## 3.2.\n",
    "\n",
    "Compute and the cosine similarity between products with asin 'B000FI4S1E', 'B000LIBUBY' and 'B000W0C07Y'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaHxSLHqItNs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session_4_Text_Representation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
