{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d23701c",
   "metadata": {},
   "source": [
    "# Evaluation of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcface96",
   "metadata": {},
   "source": [
    "Based on the same dataset used on previous weeks, let us evaluate the Collaborative Filtering (CF) models implemented last week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5b50d",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Load the test set and the predictions made with both Collaborative Filtering models in the previous session. \n",
    "2. Detect those users which are in the training set but not in the test set. Remove their predictions before evaluating the systems.\n",
    "3. Report the Root Mean Square Error (RMSE) for both CF models defined in the previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80cb0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e73a72a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 32 users in the training set that are not in the test set.\n",
      "Lengths before removing preds not in test set: 72404 72404\n",
      "After removing: 70070 70070\n",
      "\n",
      "kNN RMSE: 0.06964517083597653\n",
      "SVD RMSE: 0.07125026112605343\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TEST\n",
    "df_test = pd.read_pickle(\"testset.pkl\")[[\"reviewerID\", \"asin\", \"overall\"]]\n",
    "df_test = df_test.rename(columns={\"reviewerID\": \"uid\", \"asin\": \"iid\"})\n",
    "\n",
    "# PREDICTIONS\n",
    "nb = pd.read_pickle(\"preds1.pkl\")\n",
    "lf = pd.read_pickle(\"preds2.pkl\")\n",
    "pred_nb_list = list(nb.itertuples(index=False))\n",
    "pred_lf_list = list(lf.itertuples(index=False))\n",
    "\n",
    "\n",
    "# Detect users from training set that are not in test\n",
    "nb_users = set([pred.uid for pred in pred_nb_list])\n",
    "lf_users = set([pred.uid for pred in pred_lf_list])\n",
    "nb_users_in_pred_but_not_in_test = list(nb_users.difference(set(df_test['uid'])))\n",
    "lf_users_in_pred_but_not_in_test = list(lf_users.difference(set(df_test['uid'])))\n",
    "assert nb_users_in_pred_but_not_in_test == lf_users_in_pred_but_not_in_test\n",
    "print(f\"There are {len(lf_users_in_pred_but_not_in_test)} users in the training set that are not in the test set.\")\n",
    "\n",
    "# Remove these users' predictions for evaluation\n",
    "print(\"Lengths before removing preds not in test set:\", len(nb), len(lf))\n",
    "nb = nb[~nb.uid.isin(nb_users_in_pred_but_not_in_test)]\n",
    "lf = lf[~lf.uid.isin(nb_users_in_pred_but_not_in_test)]\n",
    "print(\"After removing:\", len(nb), len(lf))\n",
    "\n",
    "nb_merge = nb.merge(df_test, how=\"inner\", on=[\"uid\", \"iid\"])\n",
    "print(\"\\nkNN RMSE:\", np.sqrt((nb_merge[\"overall\"] - nb_merge[\"est\"])**2).mean())\n",
    "\n",
    "lf_merge = lf.merge(df_test, how=\"inner\", on=[\"uid\", \"iid\"])\n",
    "print(\"SVD RMSE:\", np.sqrt((lf_merge[\"overall\"] - lf_merge[\"est\"])**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf3c25",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Define a general method to get the top-k recommendations for each user. Print the top-k with k={5, 10} recommendations for the user with ID 'ARARUVZ8RUF5T' and its estimated ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e08574af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>est</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>B000VV1YOY</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4441</th>\n",
       "      <td>B000WR2HB6</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>B000PKKAGO</td>\n",
       "      <td>4.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4443</th>\n",
       "      <td>B000FOI48G</td>\n",
       "      <td>4.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4444</th>\n",
       "      <td>B001ET7FZE</td>\n",
       "      <td>4.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             iid    est\n",
       "4440  B000VV1YOY  5.000\n",
       "4441  B000WR2HB6  5.000\n",
       "4442  B000PKKAGO  4.750\n",
       "4443  B000FOI48G  4.675\n",
       "4444  B001ET7FZE  4.500"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5nb = nb.groupby(['uid']).apply(lambda x: x.nlargest(5,['est'])).reset_index(drop=True)[[\"uid\", \"iid\", \"est\"]]\n",
    "top5nb[top5nb[\"uid\"]==\"ARARUVZ8RUF5T\"][[\"iid\",\"est\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03325a14",
   "metadata": {},
   "source": [
    "## Excercise 3\n",
    "Report Precision@k (P@k), MAP@k and the MRR@k with k={5, 10, 20} averaged across users for both CF systems. When computing precision, we consider as relevant items those with an observed rating >= 4.0 (i.e., those items from the test set with a rating >= 4.0). Reflect on the differences obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hourly-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_column(preds, df_test, k): \n",
    "    topKpreds = preds.groupby(['uid']).apply(lambda x: x.nlargest(k,['est'])).reset_index(drop=True)[[\"uid\", \"iid\", \"est\"]]\n",
    "    merged = topKpreds.merge(df_test[[\"uid\", \"iid\", \"overall\"]], how=\"left\", on=[\"uid\", \"iid\"])\n",
    "    merged[\"relevant\"] = (merged[\"overall\"] >= 4) * 1 \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f5f89fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.004636459430979979, 0.00969441517386723)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PatK(preds, df_test, k):\n",
    "    merged = relevant_column(preds, df_test, k)\n",
    "    score  = merged[[\"uid\", \"iid\", \"relevant\"]].groupby(by=\"uid\")[\"relevant\"].mean().mean()\n",
    "    return score\n",
    "\n",
    "PatK(nb, df_test, 5), PatK(lf, df_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ae25b3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.008201615735862311, 0.036178433438707414)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MAPatK(preds, df_test, k):\n",
    "    merged = relevant_column(preds, df_test, k)\n",
    "    score  = merged[[\"uid\", \"iid\", \"relevant\"]].groupby(by=\"uid\")[\"relevant\"].apply(lambda x: 1./np.arange(1,k+1) @ x).mean()\n",
    "    return score\n",
    "\n",
    "MAPatK(nb, df_test, 5), MAPatK(lf, df_test, 5), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "entire-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5\n",
    "# preds = lf\n",
    "# topKpreds = preds.groupby(['uid']).apply(lambda x: x.nlargest(k,['est'])).reset_index(drop=True)[[\"uid\", \"iid\", \"est\"]]\n",
    "# merged = topKpreds.merge(df_test[[\"uid\", \"iid\", \"overall\"]], how=\"left\", on=[\"uid\", \"iid\"])\n",
    "# merged[\"relevant\"] = (merged[\"overall\"] >= 4) * 1 \n",
    "# merged[[\"uid\", \"iid\", \"relevant\"]].groupby(by=\"uid\").apply(first).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "checked-teaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.008201615735862311, 0.036178433438707414)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inverse of rank position of first relevant item\n",
    "def first(x):\n",
    "    for i in range(len(x)):\n",
    "        if x.iloc[i].relevant == 1:\n",
    "            return 1/(i+1)\n",
    "    return 0 \n",
    "\n",
    "#slide 47 lecture 3\n",
    "def MRRatK(preds, df_test, k):\n",
    "    merged = relevant_column(preds, df_test, k)\n",
    "    score  = merged[[\"uid\", \"iid\", \"relevant\"]].groupby(by=\"uid\").apply(first).mean()\n",
    "    return score\n",
    "\n",
    "MRRatK(nb, df_test, 5), MRRatK(lf, df_test, 5), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "spread-advertiser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.023182297154899896, 0.04847207586933614)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def HRatK(preds, df_test, k):\n",
    "    merged = relevant_column(preds, df_test, k)\n",
    "    score = merged[[\"uid\", \"iid\", \"relevant\"]].groupby(by=\"uid\")[\"relevant\"].apply(lambda x: x.any()*1).mean()\n",
    "    return score\n",
    "HRatK(nb, df_test, 5), HRatK(lf, df_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "preceding-evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            NB | LF\n",
      "  P@ 5 = 0.0046|0.0097\n",
      "MAP@ 5 = 0.0082|0.0362\n",
      "MRR@ 5 = 0.0082|0.0362\n",
      "\n",
      "  P@10 = 0.0208|0.0128\n",
      "MAP@10 = 0.0313|0.0452\n",
      "MRR@10 = 0.0313|0.0452\n",
      "\n",
      "  P@20 = 0.0455|0.0349\n",
      "MAP@20 = 0.0837|0.0834\n",
      "MRR@20 = 0.0837|0.0834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ks = [5, 10, 20]\n",
    "\n",
    "print(12*\" \" + \"NB | LF\")\n",
    "\n",
    "for k in ks:\n",
    "    P_nb, P_lf = PatK(nb, df_test, k), PatK(lf, df_test, k)\n",
    "    MAP_nb, MAP_lf = MAPatK(nb, df_test, k), MAPatK(lf, df_test, k)\n",
    "    MRR_nb, MRR_lf = MRRatK(nb, df_test, k), MRRatK(lf, df_test, k)\n",
    "    print(f\"  P@{k:2g} = {P_nb  :.4f}|{P_lf  :.4f}\")\n",
    "    print(f\"MAP@{k:2g} = {MAP_nb:.4f}|{MAP_lf:.4f}\")\n",
    "    print(f\"MRR@{k:2g} = {MRR_nb:.4f}|{MRR_lf:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4c50b",
   "metadata": {},
   "source": [
    "## Excercise 4\n",
    "\n",
    "Based on the top-5, top-10 and top-20 predictions from Exercise 2, compute the systems’ hit rate averaged over the total number of users in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "increased-tonight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023182297154899896"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = relevant_column(nb, df_test, 5)\n",
    "scores = merged[[\"uid\", \"iid\", \"relevant\"]].groupby(by=\"uid\")[\"relevant\"].apply(lambda x: x.any()*1)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "impaired-trading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            NB | LF\n",
      "MHR@ 5 = 0.0232|0.0485\n",
      "MHR@10 = 0.2076|0.1275\n",
      "MHR@20 = 0.9104|0.6976\n"
     ]
    }
   ],
   "source": [
    "ks = [5, 10, 20]\n",
    "\n",
    "print(12*\" \" + \"NB | LF\")\n",
    "\n",
    "for k in ks:\n",
    "    MHR_nb, MHR_lf = HRatK(nb, df_test, k), HRatK(lf, df_test, k)\n",
    "    print(f\"MHR@{k:2g} = {MHR_nb:.4f}|{MHR_lf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-dealing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
